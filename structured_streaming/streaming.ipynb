{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Streaming Example\n",
    "\n",
    "- Dataset: https://www.kaggle.com/datasets/ealaxi/paysim1/\n",
    "    - Synthetically created finane transactions to detect frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/19 11:40:39 WARN Utils: Your hostname, gowtham-Inspiron-5570 resolves to a loopback address: 127.0.1.1; using 192.168.29.189 instead (on interface wlp2s0)\n",
      "23/12/19 11:40:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/19 11:40:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.189:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming Process Files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f950d330280>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming Process Files\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT|  9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|    181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|    181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT| 11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  7817.71|  C90045638|      53860.0|      46042.29| M573487274|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  7107.77| C154988899|     183195.0|     176087.23| M408069119|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  7861.64|C1912850431|    176087.23|     168225.59| M633326333|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  4024.36|C1265012928|       2671.0|           0.0|M1176932104|           0.0|           0.0|      0|             0|\n",
      "|   1|   DEBIT|  5337.77| C712410124|      41720.0|      36382.23| C195600860|       41898.0|      40348.79|      0|             0|\n",
      "|   1|   DEBIT|  9644.94|C1900366749|       4465.0|           0.0| C997608398|       10845.0|     157982.12|      0|             0|\n",
      "|   1| PAYMENT|  3099.97| C249177573|      20771.0|      17671.03|M2096539129|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  2560.74|C1648232591|       5070.0|       2509.26| M972865270|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 11633.76|C1716932897|      10127.0|           0.0| M801569151|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  4098.78|C1026483832|     503264.0|     499165.22|M1635378213|           0.0|           0.0|      0|             0|\n",
      "|   1|CASH_OUT|229133.94| C905080434|      15325.0|           0.0| C476402209|        5083.0|      51513.44|      0|             0|\n",
      "|   1| PAYMENT|  1563.82| C761750706|        450.0|           0.0|M1731217984|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  1157.86|C1237762639|      21156.0|      19998.14|M1877062907|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|   671.64|C2033524545|      15123.0|      14451.36| M473053293|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER| 215310.3|C1670993182|        705.0|           0.0|C1100439041|       22425.0|           0.0|      0|             0|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('data/finance_data.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 11:43:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------+------------------+-----------+------------------+-----------------+-----------+------------------+------------------+--------------------+--------------------+\n",
      "|summary|              step|    type|            amount|   nameOrig|     oldbalanceOrg|   newbalanceOrig|   nameDest|    oldbalanceDest|    newbalanceDest|             isFraud|      isFlaggedFraud|\n",
      "+-------+------------------+--------+------------------+-----------+------------------+-----------------+-----------+------------------+------------------+--------------------+--------------------+\n",
      "|  count|           6362620| 6362620|           6362620|    6362620|           6362620|          6362620|    6362620|           6362620|           6362620|             6362620|             6362620|\n",
      "|   mean|243.39724563151657|    NULL|179861.90354913048|       NULL| 833883.1040744851|855113.6685785787|       NULL|1100701.6665196505|1224996.3982019336|0.001290820448180152| 2.51468734577894E-6|\n",
      "| stddev| 142.3319710491305|    NULL| 603858.2314629437|       NULL|2888242.6730375397|2924048.502954241|       NULL|3399180.1129944543|3674128.9421196654| 0.03590479680160438|0.001585774705736...|\n",
      "|    min|                 1| CASH_IN|               0.0|C1000000639|               0.0|              0.0|C1000004082|               0.0|               0.0|                   0|                   0|\n",
      "|    max|               743|TRANSFER|     9.244551664E7| C999999784|     5.958504037E7|    4.958504037E7| M999999784|    3.5601588935E8|    3.5617927892E8|                   1|                   1|\n",
      "+-------+------------------+--------+------------------+-----------+------------------+-----------------+-----------+------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We don't need last two columns 'isFraud' and 'isFlaggedFraud' as for streaming systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.drop('isFraud', 'isFlaggedFraud')\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step column maps a unit of time in the real world. In this case let us assume that the value of '1' in step represents 1 hour of time.\n",
    "- So we can assume for this example that we have another job that runs every hour and gets all the transactions in that time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "|  19|51352|\n",
      "|  18|49579|\n",
      "| 187|49083|\n",
      "| 235|47491|\n",
      "| 307|46968|\n",
      "| 163|46352|\n",
      "| 139|46054|\n",
      "| 403|45155|\n",
      "|  43|45060|\n",
      "| 355|44787|\n",
      "|  15|44609|\n",
      "| 186|43747|\n",
      "| 306|43615|\n",
      "|  17|43361|\n",
      "| 259|43328|\n",
      "|  16|42471|\n",
      "| 379|41759|\n",
      "|  14|41485|\n",
      "|  42|41304|\n",
      "| 354|40696|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets try to group it with respect to step and see the dataframe\n",
    "from pyspark.sql.functions import sum, col, desc\n",
    "df_pyspark.groupBy('step').count().sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can clearly see the number of transactions(count) for each step\n",
    "- We can now save the output by filtering on each step and saving it as a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code steps are not mandate but this is just for creating seperate dataframes based on each step\n",
    "# %%time\n",
    "# steps = df_pyspark.select('step').distinct().collect()\n",
    "# print(steps)\n",
    "# for step in steps:\n",
    "#     df_upt = df_pyspark.where(f'step = {step[0]}')\n",
    "#     # By addig coalesce(1) we save the dataframe to one file\n",
    "#     df_upt.coalesce(1).write.mode('append').option('header', 'true').csv('data/finance_data_upt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-011f96ea-80ac-400d-9db0-955d2a0aea30-c000.csv\n",
      "part-00000-012978cf-b20d-4756-9e0e-d3bf370d3313-c000.csv\n",
      "part-00000-05777074-0890-455d-8c1c-aac06c9660a3-c000.csv\n",
      "part-00000-059eb5c5-e2bd-404e-a0ab-47381658803e-c000.csv\n",
      "part-00000-05e25c1c-5a9a-4fcc-9a28-188be78f74ad-c000.csv\n",
      "part-00000-0836efd2-5e8f-4df4-bba9-e5099484d332-c000.csv\n",
      "part-00000-092ca57f-ec0e-4e7f-ab54-215d12f4c200-c000.csv\n",
      "part-00000-0c5cf012-b6d7-4eb5-bcaf-a5458b2cbfe1-c000.csv\n",
      "part-00000-10eb07cb-a846-45cf-a2f3-c951ae37802a-c000.csv\n",
      "part-00000-1264dacc-c225-461e-9054-0c7930c3b437-c000.csv\n",
      "part-00000-13be92a5-58e6-45be-8b2b-7a5036f862ce-c000.csv\n",
      "part-00000-1976a447-5384-48be-8b3d-8b20b1fd86c5-c000.csv\n",
      "part-00000-1afe4d59-5646-4f4a-a8a5-030d1709af88-c000.csv\n",
      "part-00000-1cab8aaf-ff48-49a4-9e75-ca3d7d2cd5bb-c000.csv\n",
      "part-00000-21324b12-3f58-4a6b-a084-57b474b17943-c000.csv\n",
      "part-00000-2240c379-2754-4242-8ca6-a8333aed925c-c000.csv\n",
      "part-00000-301cf07e-511a-425d-be75-621fee5ca89d-c000.csv\n",
      "part-00000-3687259a-1306-403d-893e-dc1d4666c699-c000.csv\n",
      "part-00000-392b8888-b610-48d5-bf35-31b3c4c59ae4-c000.csv\n",
      "part-00000-43bfe1d3-c8d7-4f46-b431-1520d5361450-c000.csv\n",
      "part-00000-454cfd66-d4bc-4908-8c06-81456cb2da53-c000.csv\n",
      "part-00000-4c4a07a5-fdcd-4f6e-8d57-d0a9606d2874-c000.csv\n",
      "part-00000-4c899735-cbed-44bf-bfd9-9053b50e0eb2-c000.csv\n",
      "part-00000-4f5a718b-b0ea-4219-95f6-23110cb9792d-c000.csv\n",
      "part-00000-50ec9cf6-4990-429f-965d-d37fac64b9aa-c000.csv\n",
      "part-00000-5296f1c6-f94a-46b8-8398-c3d6bcb230de-c000.csv\n",
      "part-00000-563ea67c-e425-4a47-8dfe-c08878026d55-c000.csv\n",
      "part-00000-5d5e8643-cfd3-4778-80a9-585b34a8fb53-c000.csv\n",
      "part-00000-66e679ad-62d2-4e8d-92ed-16d69a15f78b-c000.csv\n",
      "part-00000-68cff3c1-2f2d-4c83-9a60-159ec7be1cbf-c000.csv\n",
      "part-00000-6da0bc1e-7859-485b-96c4-fe197b55b33c-c000.csv\n",
      "part-00000-6f2137f3-7662-4a9f-a1fa-764a7adced46-c000.csv\n",
      "part-00000-742b7fcf-c75d-419d-9457-0bde95dbc228-c000.csv\n",
      "part-00000-7b2bc2a7-376c-4f35-9676-f307c9d6d9fc-c000.csv\n",
      "part-00000-7bef6143-d819-47f5-a248-78b89ffff446-c000.csv\n",
      "part-00000-7cdcd8b1-e156-4f2b-afe4-0fbbb2e4564a-c000.csv\n",
      "part-00000-81149f57-8224-40a9-b7a3-85343a0a7296-c000.csv\n",
      "part-00000-82640b8d-b195-4ace-a19c-32fdb5119b40-c000.csv\n",
      "part-00000-85f566ec-0b27-42ae-97dd-de6b5fa2d59d-c000.csv\n",
      "part-00000-864eaf67-c0bf-4b20-8456-20882b3222e2-c000.csv\n",
      "part-00000-8e2dc961-4e0a-43dc-8ed0-6eeb874a087d-c000.csv\n",
      "part-00000-8f3f5562-057c-4c17-8301-f87bcc61ba1d-c000.csv\n",
      "part-00000-8f84312f-dd7a-4922-b93c-f3a91e995215-c000.csv\n",
      "part-00000-932bcd9e-5465-4295-b768-dc0985c8dc8b-c000.csv\n",
      "part-00000-9e1df954-870e-47a9-bbac-e3070c67217c-c000.csv\n",
      "part-00000-a25d4a02-ea9c-4afa-b370-7b01867d0a47-c000.csv\n",
      "part-00000-a9a812cf-ac88-43f4-a945-917f5413afd1-c000.csv\n",
      "part-00000-abfee19f-9558-4fed-af3d-b844c7057ab8-c000.csv\n",
      "part-00000-acd74c6d-2554-4901-a797-46eda8bd0ca8-c000.csv\n",
      "part-00000-afbd222c-0d62-40c8-a788-8a1dd805f64e-c000.csv\n",
      "part-00000-b3a95fa4-6621-49db-91e9-87cd539e994e-c000.csv\n",
      "part-00000-baefaa2d-1864-4d87-93a0-401345afe072-c000.csv\n",
      "part-00000-bdc1cd16-411e-471b-9b17-153a3be6a99a-c000.csv\n",
      "part-00000-be8a432e-2505-442b-8cbd-2d4a9bac3099-c000.csv\n",
      "part-00000-bf0a09fb-2df3-4286-b3b3-905a12954adc-c000.csv\n",
      "part-00000-c0ee3b70-54a1-49f7-8dae-d8bcd14c7fc2-c000.csv\n",
      "part-00000-c5c2650f-33b5-4a20-98d6-cb30048a766f-c000.csv\n",
      "part-00000-c5ee75d1-6282-415c-92d2-273d607ef8be-c000.csv\n",
      "part-00000-c64c550f-87e7-4555-85b1-b89bfd7d2fd9-c000.csv\n",
      "part-00000-cc386eaf-1308-42e3-9104-fe42dc7a32e3-c000.csv\n",
      "part-00000-cd996661-2da0-475a-8187-e54b8859424e-c000.csv\n",
      "part-00000-ce546bed-4aa2-42df-864f-41330c113f32-c000.csv\n",
      "part-00000-d43a8f3d-a120-49ab-a1b5-b72064e94a8a-c000.csv\n",
      "part-00000-dc8ba13f-cfab-42af-9045-f1e50ce1e7a2-c000.csv\n",
      "part-00000-dd41ff8f-15c6-45d6-a01f-5de461aee849-c000.csv\n",
      "part-00000-e2dd731c-da92-4c1a-8974-365396feb55d-c000.csv\n",
      "part-00000-e3bb268f-8d9c-472d-8938-302d0cfce424-c000.csv\n",
      "part-00000-e9ea39bb-4f22-4c34-8b5e-1604d1cd7d9d-c000.csv\n",
      "part-00000-f4554cd0-3496-49a4-be3f-1a54dc746755-c000.csv\n",
      "part-00000-f502ac09-c8ca-41ec-84b3-228e7ba2f4b0-c000.csv\n",
      "part-00000-f72d2e2c-6cd7-4958-8752-aeec28aecc6d-c000.csv\n",
      "part-00000-ff86d16e-ca35-4a4f-924e-5e5ccf2048c4-c000.csv\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# The below line of code displays the all the dataframes that were saved in the above step\n",
    "!cd data/finance_data_upt/ && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|  30|TRANSFER|122235.11|C1279213624|    122235.11|           0.0|C2100791242|           0.0|           0.0|\n",
      "|  30|CASH_OUT|122235.11| C356571967|    122235.11|           0.0|C1905066109|           0.0|     122235.11|\n",
      "|  30|TRANSFER|236947.25|C1273047168|    236947.25|           0.0|C1636242484|           0.0|           0.0|\n",
      "|  30|CASH_OUT|236947.25| C474899128|    236947.25|           0.0| C116505755|      70084.28|     307031.53|\n",
      "|  30|TRANSFER| 19016.22| C182136548|     19016.22|           0.0| C717672329|           0.0|           0.0|\n",
      "|  30|CASH_OUT| 19016.22|C1257554854|     19016.22|           0.0|C1044117371|    9347999.52|    9367015.74|\n",
      "|  30|TRANSFER|109401.88|C2037269206|    109401.88|           0.0| C337579043|           0.0|           0.0|\n",
      "|  30|CASH_OUT|109401.88| C984153855|    109401.88|           0.0|C1806203979|     245588.62|      354990.5|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partition = spark.read.csv(\n",
    "            'data/finance_data_upt/part-00000-4c4a07a5-fdcd-4f6e-8d57-d0a9606d2874-c000.csv',\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    ")\n",
    "df_partition.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "|  30|    8|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partition.groupBy('step').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a streaming version of the input: we will read each file one by one as if it was a stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('step', IntegerType(), True), StructField('type', StringType(), True), StructField('amount', DoubleType(), True), StructField('nameOrig', StringType(), True), StructField('oldbalanceOrg', DoubleType(), True), StructField('newbalanceOrig', DoubleType(), True), StructField('nameDest', StringType(), True), StructField('oldbalanceDest', DoubleType(), True), StructField('newbalanceDest', DoubleType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the schema of the partition\n",
    "dataSchema = df_partition.schema\n",
    "dataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here streaming data is created from the finance_data folder setting the trigger to '1'\n",
    "# maxFilesPerTrigger - allows to control how quickly spark will reall the files in the folder\n",
    "stream = (\n",
    "        spark.readStream.schema(dataSchema) \\\n",
    "        .option('maxFilesPerTrigger',1) \\\n",
    "        .csv('data/finance_data_upt/')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us perform some transformations on the streaming data(Dstreams)\n",
    "- Here 'nameDest' column is the 'recipient ID' of the transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea is to identify the fradulent transactions, \n",
    "# Considering a scenario - if multiple account holders transfer the amount to one account\n",
    "# So grouping the 'nameDest' feature and sorting it\n",
    "from pyspark.sql.functions import sum, col, desc\n",
    "dest_count = stream.groupBy('nameDest').count().orderBy(desc('count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transformation \n",
    "- We need to specify the output sink for the results. For this example we are going to write it to a memory sink which keeps the results in the memory.\n",
    "- We also need to denfine how spark will output that data. In this example we will use the complete output mode(rewriting all the keys along with their counts after every trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 11:59:06 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fcb82d61-07c0-4b8c-8f24-9bb769732811. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/18 11:59:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C357028200|    5|\n",
      "|C1308497108|    5|\n",
      "| C553146000|    5|\n",
      "| C479356446|    5|\n",
      "| C480863671|    5|\n",
      "|C2030023933|    5|\n",
      "| C850125606|    4|\n",
      "| C823524092|    4|\n",
      "|C1010390776|    4|\n",
      "| C347981017|    4|\n",
      "| C685802025|    4|\n",
      "|C1652671483|    4|\n",
      "|C1353518278|    4|\n",
      "|C1120829019|    4|\n",
      "|C1954345491|    4|\n",
      "|C1924761929|    4|\n",
      "| C860128720|    4|\n",
      "|C1668323738|    4|\n",
      "|C1919331419|    4|\n",
      "|C1007286597|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C357028200|    5|\n",
      "|C1308497108|    5|\n",
      "| C553146000|    5|\n",
      "| C479356446|    5|\n",
      "| C480863671|    5|\n",
      "|C2030023933|    5|\n",
      "| C850125606|    4|\n",
      "| C823524092|    4|\n",
      "|C1010390776|    4|\n",
      "| C347981017|    4|\n",
      "| C685802025|    4|\n",
      "|C1652671483|    4|\n",
      "|C1353518278|    4|\n",
      "|C1120829019|    4|\n",
      "|C1954345491|    4|\n",
      "|C1924761929|    4|\n",
      "| C860128720|    4|\n",
      "|C1668323738|    4|\n",
      "|C1919331419|    4|\n",
      "|C1007286597|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1954345491|    5|\n",
      "| C357028200|    5|\n",
      "|C1308497108|    5|\n",
      "| C553146000|    5|\n",
      "| C479356446|    5|\n",
      "| C480863671|    5|\n",
      "|C2030023933|    5|\n",
      "| C850125606|    4|\n",
      "| C823524092|    4|\n",
      "|C1010390776|    4|\n",
      "| C347981017|    4|\n",
      "|C1192879985|    4|\n",
      "| C685802025|    4|\n",
      "|C1652671483|    4|\n",
      "|C1353518278|    4|\n",
      "|C1120829019|    4|\n",
      "|C1924761929|    4|\n",
      "| C860128720|    4|\n",
      "|C1668323738|    4|\n",
      "| C794978521|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1954345491|    5|\n",
      "| C357028200|    5|\n",
      "|C1308497108|    5|\n",
      "| C553146000|    5|\n",
      "| C479356446|    5|\n",
      "| C480863671|    5|\n",
      "|C2030023933|    5|\n",
      "| C850125606|    4|\n",
      "| C823524092|    4|\n",
      "|C1010390776|    4|\n",
      "| C347981017|    4|\n",
      "|C1192879985|    4|\n",
      "| C685802025|    4|\n",
      "|C1652671483|    4|\n",
      "|C1353518278|    4|\n",
      "|C1120829019|    4|\n",
      "|C1924761929|    4|\n",
      "| C860128720|    4|\n",
      "|C1668323738|    4|\n",
      "| C794978521|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C499714286|   10|\n",
      "|C2131465140|    9|\n",
      "|C1292435170|    8|\n",
      "|C1437074823|    8|\n",
      "| C240779865|    8|\n",
      "|C1677262317|    8|\n",
      "|C2067466390|    8|\n",
      "| C873744218|    8|\n",
      "|C1885272526|    8|\n",
      "|C1730752455|    8|\n",
      "| C141847315|    8|\n",
      "|C1876731592|    7|\n",
      "| C131837504|    7|\n",
      "| C668247389|    7|\n",
      "| C502295547|    7|\n",
      "|C1979563377|    7|\n",
      "|C1229206841|    7|\n",
      "|C1673688244|    7|\n",
      "|C2040491146|    7|\n",
      "| C776749939|    7|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C499714286|   10|\n",
      "| C115095474|   10|\n",
      "|C2131465140|    9|\n",
      "|C1401721023|    9|\n",
      "|C1876731592|    8|\n",
      "|C1229206841|    8|\n",
      "|C1292435170|    8|\n",
      "|C1437074823|    8|\n",
      "| C859300166|    8|\n",
      "| C240779865|    8|\n",
      "|C1239707538|    8|\n",
      "|C1638556416|    8|\n",
      "|C1139127799|    8|\n",
      "|C1677262317|    8|\n",
      "|C1499819931|    8|\n",
      "|C2067466390|    8|\n",
      "|C1774146551|    8|\n",
      "| C873744218|    8|\n",
      "|C1382816537|    8|\n",
      "|C1885272526|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1590550415|   35|\n",
      "| C985934102|   33|\n",
      "| C564160838|   30|\n",
      "|C2083562754|   26|\n",
      "| C665576141|   25|\n",
      "| C401424608|   23|\n",
      "|C1286084959|   23|\n",
      "|  C33524623|   23|\n",
      "| C998351292|   23|\n",
      "|C1023714065|   22|\n",
      "| C451111351|   21|\n",
      "|C1789550256|   21|\n",
      "| C747464370|   20|\n",
      "|C1068824137|   19|\n",
      "| C932583850|   19|\n",
      "| C453211571|   19|\n",
      "|C1360767589|   19|\n",
      "| C248609774|   18|\n",
      "|C1740000325|   18|\n",
      "|C1335050193|   18|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "| C985934102|   36|\n",
      "|C1590550415|   36|\n",
      "| C564160838|   30|\n",
      "|C2083562754|   28|\n",
      "| C665576141|   26|\n",
      "| C998351292|   26|\n",
      "|C1789550256|   25|\n",
      "| C401424608|   23|\n",
      "| C451111351|   23|\n",
      "|C1286084959|   23|\n",
      "|  C33524623|   23|\n",
      "|C1023714065|   22|\n",
      "|C1360767589|   21|\n",
      "|C1068824137|   20|\n",
      "|C1170794006|   20|\n",
      "| C453211571|   20|\n",
      "|C1749186397|   20|\n",
      "| C747464370|   20|\n",
      "| C248609774|   19|\n",
      "| C932583850|   19|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.              (39 + 8) / 200]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gowtham/Documents/Python/BigData/spark_demo/spark-streaming/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/gowtham/Documents/Python/BigData/spark_demo/spark-streaming/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 104:=======>     (122 + 8) / 200][Stage 105:>                (0 + 0) / 8]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gowtham/Documents/Python/BigData/spark_demo/streaming.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gowtham/Documents/Python/BigData/spark_demo/streaming.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gowtham/Documents/Python/BigData/spark_demo/streaming.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     df_query \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msql(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gowtham/Documents/Python/BigData/spark_demo/streaming.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSELECT * FROM dest_counts WHERE nameDest != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnameDest\u001b[39m\u001b[39m'\u001b[39m\u001b[39m AND count >= 2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gowtham/Documents/Python/BigData/spark_demo/streaming.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gowtham/Documents/Python/BigData/spark_demo/streaming.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m df_query\u001b[39m.\u001b[39;49mcount() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gowtham/Documents/Python/BigData/spark_demo/streaming.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         df_query\u001b[39m.\u001b[39mshow(\u001b[39m20\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gowtham/Documents/Python/BigData/spark_demo/streaming.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Python/BigData/spark_demo/spark-streaming/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1212\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \n\u001b[1;32m   1214\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcount())\n",
      "File \u001b[0;32m~/Documents/Python/BigData/spark_demo/spark-streaming/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Documents/Python/BigData/spark_demo/spark-streaming/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Documents/Python/BigData/spark_demo/spark-streaming/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 114:==================>                                   (67 + 8) / 200]\r"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "        dest_count.writeStream.queryName('dest_counts') \\\n",
    "        .format('memory') \\\n",
    "        .outputMode('complete') \\\n",
    "        .start()\n",
    ")\n",
    "# query.awaitTermination()        # can be used in production environment\n",
    "import time\n",
    "for x in range(50):\n",
    "    df_query = spark.sql(\n",
    "            \"SELECT * FROM dest_counts WHERE nameDest != 'nameDest' AND count >= 2\"\n",
    "    )\n",
    "    if df_query.count() > 0:\n",
    "        df_query.show(20)\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 114:==================================================>  (192 + 8) / 200]\r"
     ]
    }
   ],
   "source": [
    "spark.streams.active[0].isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 12:00:55 WARN Shell: Interrupted while joining on: Thread[Thread-48857,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:511)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:511)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/12/18 12:00:55 WARN Shell: Interrupted while joining on: Thread[Thread-48856,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:511)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:511)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/12/18 12:00:55 WARN Shell: Interrupted while joining on: Thread[Thread-48859,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:511)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:511)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/12/18 12:00:55 WARN Shell: Interrupted while joining on: Thread[Thread-48855,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:511)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:511)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/12/18 12:00:55 WARN Shell: Interrupted while joining on: Thread[Thread-48861,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:511)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:511)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/12/18 12:00:55 WARN Shell: Interrupted while joining on: Thread[Thread-48858,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:511)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:511)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/12/18 12:00:55 WARN TaskSetManager: Lost task 17.0 in stage 117.0 (TID 5029) (192.168.29.189 executor driver): TaskKilled (Stage cancelled: Job 64 cancelled part of cancelled job group efa1207b-e8d5-4619-8471-7dde54b9f9f1)\n",
      "23/12/18 12:00:55 WARN TaskSetManager: Lost task 23.0 in stage 117.0 (TID 5035) (192.168.29.189 executor driver): TaskKilled (Stage cancelled: Job 64 cancelled part of cancelled job group efa1207b-e8d5-4619-8471-7dde54b9f9f1)\n",
      "23/12/18 12:00:55 WARN TaskSetManager: Lost task 20.0 in stage 117.0 (TID 5032) (192.168.29.189 executor driver): TaskKilled (Stage cancelled: Job 64 cancelled part of cancelled job group efa1207b-e8d5-4619-8471-7dde54b9f9f1)\n",
      "23/12/18 12:00:55 WARN TaskSetManager: Lost task 19.0 in stage 117.0 (TID 5031) (192.168.29.189 executor driver): TaskKilled (Stage cancelled: Job 64 cancelled part of cancelled job group efa1207b-e8d5-4619-8471-7dde54b9f9f1)\n",
      "23/12/18 12:00:55 WARN TaskSetManager: Lost task 22.0 in stage 117.0 (TID 5034) (192.168.29.189 executor driver): TaskKilled (Stage cancelled: Job 64 cancelled part of cancelled job group efa1207b-e8d5-4619-8471-7dde54b9f9f1)\n",
      "23/12/18 12:00:55 WARN TaskSetManager: Lost task 21.0 in stage 117.0 (TID 5033) (192.168.29.189 executor driver): TaskKilled (Stage cancelled: Job 64 cancelled part of cancelled job group efa1207b-e8d5-4619-8471-7dde54b9f9f1)\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-streaming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
